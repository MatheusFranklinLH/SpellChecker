{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpellChecker.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM20bTi3SbxBJL+tkd7SURo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatheusFranklinLH/SpellChecker/blob/main/SpellChecker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyysyDs8eg7X"
      },
      "source": [
        "with open(\"artigos.txt\", \"r\") as f: #Read file artigos.txt\n",
        "  articles = f.read() \n",
        "\n",
        "print(articles[:500]) #Corpus in portuguese"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZPpHVAbjTJc"
      },
      "source": [
        "print(len(articles))\n",
        "len(\"Hello\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUgjUZI0k258"
      },
      "source": [
        "text_example = \"Hello, how are you?\" # Just a example\n",
        "tokens = text_example.split()\n",
        "print(tokens)\n",
        "len(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdOhzsDMxp48"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AL2aje2vkrGM"
      },
      "source": [
        "splited_words = nltk.tokenize.word_tokenize(text_example)\n",
        "print(splited_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpIBSGv6zD1v"
      },
      "source": [
        "'word'.isalpha() #String methods: return true if all characters are alphabetics (letters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sRxckoPyEPi"
      },
      "source": [
        "def split_words(token_list): # Returns only word tokens\n",
        "  word_list = []\n",
        "  for token in token_list:\n",
        "    if token.isalpha():\n",
        "      word_list.append(token)\n",
        "  return word_list\n",
        "\n",
        "print(split_words(splited_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptxrOhS5zqjW"
      },
      "source": [
        "token_list = nltk.tokenize.word_tokenize(articles)\n",
        "words_list = split_words(token_list)\n",
        "print(f\"Number of words in corpus: {len(words_list)}  \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enxleunN0VhV"
      },
      "source": [
        "print(words_list[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9_ukwo01mK4"
      },
      "source": [
        "def normalize(words_list):\n",
        "  normalized_list = []\n",
        "  for word in words_list:\n",
        "    normalized_list.append(word.lower()) # lower turns every letter to lowercase\n",
        "  return normalized_list"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cFwofUR2MM5"
      },
      "source": [
        "normalized_list = normalize(words_list)\n",
        "print(normalized_list[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeMq18tt2Ugq"
      },
      "source": [
        "set([1,2,3,3,3,4,5,6]) # Eliminate repetition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMJbqYLy4G4s",
        "outputId": "ff8820f5-0882-45c2-a655-11ef40d45389"
      },
      "source": [
        "print(f\"Actual amount of words: {len(set(normalized_list))}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual amount of words: 17652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHYBU86Q8W4A"
      },
      "source": [
        "list = \"lgic\"\n",
        "(list[:1], list[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQtuEBzJ8AXq"
      },
      "source": [
        "word_example = \"lgic\" #Logic misspelled\n",
        "\n",
        "def insert_letters(slices):\n",
        "  new_words = []\n",
        "  letters = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
        "  for L, R in slices:\n",
        "    for letter in letters:\n",
        "      new_words.append(L+letter+R)\n",
        "  return new_words\n",
        "\n",
        "def word_generator(word):\n",
        "  slices = []\n",
        "  for i in range(len(word)+1):\n",
        "    slices.append((word[:i],word[i:]))\n",
        "  generated_words = insert_letters(slices)\n",
        "  return generated_words\n",
        "\n",
        "gen_words = word_generator(word_example)\n",
        "print(gen_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV1Ouy52_s0K"
      },
      "source": [
        "def spell_checker(word):\n",
        "  gen_words = word_generator(word)\n",
        "  correct_word = max(gen_words, key=probability) #Return the most likely word\n",
        "  return correct_word\n",
        "  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQujvT7CCHAR"
      },
      "source": [
        "frequency = nltk.FreqDist(normalized_list)\n",
        "total_amount = len(normalized_list)\n",
        "frequency.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_ym-m7iCvWe"
      },
      "source": [
        "def probability(gen_word):\n",
        "  return frequency[gen_word]/total_amount\n",
        "\n",
        "probability(\"logica\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huhFp8cRDqD5"
      },
      "source": [
        "spell_checker(\"lgica\") # lógica means logic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDRwU3u2gxSY"
      },
      "source": [
        "def delete_letters(slices):\n",
        "  new_words = []\n",
        "  for L, R in slices:\n",
        "      new_words.append(L+R[1:])\n",
        "  return new_words\n",
        "\n",
        "def change_letters(slices):\n",
        "  new_words = []\n",
        "  letters = 'abcdefghijklmnopqrstuvwxyzàáâãèéêìíîòóôõùúûç'\n",
        "  for L, R in slices:\n",
        "    for letter in letters:\n",
        "      new_words.append(L+letter+R[1:])\n",
        "  return new_words\n",
        "\n",
        "def reverse_letters(slices):\n",
        "  new_words = []\n",
        "  for L, R in slices:\n",
        "    if(len(R)>=2):\n",
        "      new_words.append(L+R[1]+R[0]+R[2:])\n",
        "  return new_words\n",
        "\n",
        "def word_generator(word):\n",
        "  slices = []\n",
        "  for i in range(len(word)+1):\n",
        "    slices.append((word[:i],word[i:]))\n",
        "  generated_words = insert_letters(slices)\n",
        "  generated_words += delete_letters(slices)\n",
        "  generated_words += change_letters(slices)\n",
        "  generated_words += reverse_letters(slices)\n",
        "  return generated_words\n",
        "\n",
        "gen_words = word_generator(\"lgóica\")\n",
        "print(gen_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sk8jsG_fqsC6"
      },
      "source": [
        "def create_test_data(file_name): #The test samples are arranged in lines where first comes the correct word and then misspeled\n",
        "  test_word_list = []\n",
        "  f = open(file_name, \"r\")\n",
        "  for line in f:\n",
        "    correct, wrong = line.split()\n",
        "    test_word_list.append((correct, wrong))\n",
        "  f.close()\n",
        "  return test_word_list\n",
        "\n",
        "test_list = create_test_data(\"palavras.txt\")\n",
        "print(test_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox1TlqYTmHYx",
        "outputId": "254eb621-4cfd-4b63-d4a2-4f4b477aaf68"
      },
      "source": [
        "def rater(tests, vocabulary):\n",
        "  total_words = len(tests)\n",
        "  hit = 0\n",
        "  unknown = 0\n",
        "  for correct, wrong in tests:\n",
        "    if spell_checker(wrong) == correct:\n",
        "      hit += 1\n",
        "    else:\n",
        "      unknown += (correct not in vocabulary)\n",
        "  hit_rate = round(hit * 100 / total_words, 2)\n",
        "  unknown_rate = round(unknown * 100 / total_words, 2)\n",
        "  print(f\"Hit rate: {hit_rate}% of {total_words} words, unknown words rate: {unknown_rate}%\")\n",
        "\n",
        "vocabulary = set(normalized_list)\n",
        "rater(test_list, vocabulary)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit rate: 76.34% of 186 words, unknown words rate: 6.99%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gDAPBaKp3pZ"
      },
      "source": [
        "word = \"lóiigica\"\n",
        "\n",
        "def turbo_generator(gen_words):\n",
        "  new_words = []\n",
        "  for word in gen_words:\n",
        "    new_words += word_generator(word)\n",
        "  return new_words\n",
        "\n",
        "g_words = turbo_generator(word_generator(word))\n",
        "\"lógica\" in g_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYldTnwlsdOn"
      },
      "source": [
        "len(g_words) # Too much samples for only one word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr_fDPvMslZ7"
      },
      "source": [
        "def new_spell_checker(word, vocabulary):\n",
        "  gen_words = word_generator(word)\n",
        "  turbo_gen_words = turbo_generator(gen_words)\n",
        "  words = set(gen_words + turbo_gen_words)\n",
        "  candidates = [word]\n",
        "  for w in words:\n",
        "    if w in vocabulary:\n",
        "      candidates.append(w)\n",
        "  #print(len(candidates))\n",
        "  correct_word = max(candidates, key=probability) #Return the most likely word\n",
        "  return correct_word\n",
        "\n",
        "new_spell_checker(\"lóiigica\", vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR0madm8ufS6"
      },
      "source": [
        "def rater(tests, vocabulary):\n",
        "  total_words = len(tests)\n",
        "  hit = 0\n",
        "  unknown = 0\n",
        "  for correct, wrong in tests:\n",
        "    checked_word = new_spell_checker(wrong, vocabulary)\n",
        "    unknown += (correct not in vocabulary)\n",
        "    if  checked_word == correct:\n",
        "      hit += 1\n",
        "    #else:\n",
        "      #print(wrong + \" - \" + spell_checker(wrong) + \" - \" + checked_word)\n",
        "  hit_rate = round(hit * 100 / total_words, 2)\n",
        "  unknown_rate = round(unknown * 100 / total_words, 2)\n",
        "  print(f\"Hit rate: {hit_rate}% of {total_words} words, unknown words rate: {unknown_rate}%\")\n",
        "\n",
        "vocabulary = set(normalized_list)\n",
        "rater(test_list, vocabulary)\n",
        "\n",
        "##Turbo checker had a worse perform than the spell checker"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I43WoswIvZuK",
        "outputId": "53b76d3c-c4f7-44ef-89f4-efa0cf8c19ae"
      },
      "source": [
        "word = \"lgica\"\n",
        "print(new_spell_checker(word, vocabulary))\n",
        "print(spell_checker(word))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fica\n",
            "lógica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY0XV4xpybVF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}